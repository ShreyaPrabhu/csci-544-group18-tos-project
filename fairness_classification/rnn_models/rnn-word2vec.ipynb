{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "268d41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies to run the notebook\n",
    "\n",
    "# !pip install torch==1.12.1\n",
    "# !pip install torchmetrics==0.10.2\n",
    "# !pip install torchvision==0.14.0\n",
    "# !pip install texttable==1.6.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fef83eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from texttable import Texttable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7a03ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_NAME = \"word2vec\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1167ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \".\"\n",
    "TRAIN_EMBEDDINGS = f\"../data/word2vec_train.pkl\"\n",
    "TEST_EMBEDDINGS = f\"../data/word2vec_test.pkl\"\n",
    "TRAIN_DATASET_PATH = f\"{BASE_PATH}/../legal_bert/data/tos_clauses_train.csv\"\n",
    "TEST_DATASET_PATH = f\"{BASE_PATH}/../legal_bert/data/tos_clauses_dev.csv\"\n",
    "RNN_MODEL_PATH = f\"{BASE_PATH}/models/rnn_{EMBEDDINGS_NAME}_model.pt\"\n",
    "GRU_MODEL_PATH = f\"{BASE_PATH}/models/gru_{EMBEDDINGS_NAME}_model.pt\"\n",
    "LSTM_MODEL_PATH = f\"{BASE_PATH}/models/lstm_{EMBEDDINGS_NAME}_model.pt\"\n",
    "BILSTM_MODEL_PATH = f\"{BASE_PATH}/models/bilstm_{EMBEDDINGS_NAME}_model.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf44010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train BERT embeddings\n",
    "with open(TRAIN_EMBEDDINGS, \"rb\") as f:\n",
    "    training_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e45b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test BERT embeddings\n",
    "with open(TEST_EMBEDDINGS, \"rb\") as f:\n",
    "    testing_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2bd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check\n",
    "# for item in training_data:\n",
    "#     print(\"The data is : \", item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dca0767b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7531, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data), len(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1481e82e",
   "metadata": {},
   "source": [
    "<span style=\"color:darkviolet\">\n",
    "<font size=\"4\">Get the labels from train and test files.</font>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ec7097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = \"../legal_bert/data/tos_clauses_train.csv\"\n",
    "TEST_DATASET_PATH = \"../legal_bert/data/tos_clauses_dev.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7feb1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATASET_PATH, header=0)\n",
    "test_df = pd.read_csv(TEST_DATASET_PATH, header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a29d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = train_df.label.values\n",
    "test_targets = test_df.label.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "541edf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0378fa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92206a0",
   "metadata": {},
   "source": [
    "<span style=\"color:darkviolet\">\n",
    "<font size=\"4\">Create Dataset, Train and Test Classes</font>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efaf4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "    All other datasets should subclass it. All subclasses should\n",
    "    override ``__len__``, that provides the size of the dataset,\n",
    "    and ``__getitem__``, supporting integer indexing in range\n",
    "    from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bb78e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TOSDataset(Dataset):\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        self.data1 = X\n",
    "        self.data2 = Y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data1[index]\n",
    "        y = self.data2[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            x = torch.tensor(x)\n",
    "\n",
    "        return torch.tensor(x), torch.tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d37e581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.sampler.SubsetRandomSampler object at 0x7fb460496fe0>\n",
      "train_fair:6705\n",
      "train_unfair:826\n"
     ]
    }
   ],
   "source": [
    "test_len = len(test_df)\n",
    "train_len = len(train_df)\n",
    "X_train_tensor = TOSDataset(train_df[\"sentences\"], train_df[\"label\"])\n",
    "# X_test_tensor = Train_Model(test_df)\n",
    "\n",
    "num_train = len(X_train_tensor)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "# split = int(np.floor(num_train))\n",
    "# train_idx = indices[split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(indices)\n",
    "# valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "print(train_sampler)\n",
    "train_df_by_index = train_df.loc[indices]\n",
    "# val_df_by_index = df_train.loc[valid_idx]\n",
    "train_fair = sum(train_df_by_index[\"label\"] == 0)\n",
    "train_unfair = sum(train_df_by_index[\"label\"] == 1)\n",
    "# val_fair = sum(val_df_by_index['label'] == 0)\n",
    "# val_unfair = sum(val_df_by_index['label'] == 1)\n",
    "\n",
    "print(\"train_fair:\" + str(train_fair))\n",
    "print(\"train_unfair:\" + str(train_unfair))\n",
    "# print(\"val_fair:\" + str(val_fair))\n",
    "# print(\"val_unfair:\" + str(val_unfair))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cca2a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TOSDataset(training_data, train_targets, transform=transforms.ToTensor())\n",
    "test_data = TOSDataset(testing_data, test_targets, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b7413",
   "metadata": {},
   "source": [
    "<span style=\"color:darkviolet\">\n",
    "<font size=\"4\">Prepare Data loaders</font>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db13f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many samples per batch to load\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "NUM_WORKERS = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14d9572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36811bd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_SingleProcessDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check sizes\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_loader)\n\u001b[0;32m----> 3\u001b[0m sample_x, sample_y \u001b[38;5;241m=\u001b[39m \u001b[43mdataiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample input size: \u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_x\u001b[38;5;241m.\u001b[39msize())  \u001b[38;5;66;03m# batch_size, seq_length\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(\"Sample input: \\n\", sample_x)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_SingleProcessDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "# check sizes\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print(\"Sample input size: \", sample_x.size())  # batch_size, seq_length\n",
    "# print(\"Sample input: \\n\", sample_x)\n",
    "print()\n",
    "print(\"Sample label size: \", sample_y.size())  # batch_size\n",
    "# print(\"Sample label: \\n\", sample_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f98c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.squeeze(sample_x, dim=1).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccefed",
   "metadata": {},
   "source": [
    "<span style=\"color:darkviolet\">\n",
    "<font size=\"5\">SIMPLE RNN</font><br>\n",
    "<font size=\"2.5\">Number of hidden dimension : 20</font> <br>\n",
    "<font size=\"2.5\">Number of layers: 1</font> <br>\n",
    "<font size=\"2.5\">Number of epochs: 5</font> <br>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f35604d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "\n",
    "        super(RNNet, self).__init__()\n",
    "\n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=3, batch_first=True, nonlinearity=\"relu\")\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(3, self.hidden_dim))\n",
    "        h0 = h0.to(device)\n",
    "        # One time step\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4a1f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b69d3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "OUTPUT_DIM = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5c62989",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_HIDDEN_DIM = 20\n",
    "RNN_N_EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acb233cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56159582, 4.55871671])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weight = compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(train_df_by_index[\"label\"]), y=train_df_by_index[\"label\"]\n",
    ")\n",
    "class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53e1f120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.032834 \\Test Loss: 0.029671\n",
      "Test loss decreased (inf --> 0.029671). Saving model...\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.023768 \\Test Loss: 0.023491\n",
      "Test loss decreased (0.029671 --> 0.023491). Saving model...\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.020612 \\Test Loss: 0.022441\n",
      "Test loss decreased (0.023491 --> 0.022441). Saving model...\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.019368 \\Test Loss: 0.021934\n",
      "Test loss decreased (0.022441 --> 0.021934). Saving model...\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.018702 \\Test Loss: 0.023604\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.017162 \\Test Loss: 0.024530\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.016412 \\Test Loss: 0.021022\n",
      "Test loss decreased (0.021934 --> 0.021022). Saving model...\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.015720 \\Test Loss: 0.018891\n",
      "Test loss decreased (0.021022 --> 0.018891). Saving model...\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.015538 \\Test Loss: 0.019831\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.014522 \\Test Loss: 0.018548\n",
      "Test loss decreased (0.018891 --> 0.018548). Saving model...\n"
     ]
    }
   ],
   "source": [
    "model = RNNet(EMBEDDING_DIM, RNN_HIDDEN_DIM, OUTPUT_DIM)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weight))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "test_min_loss = np.inf\n",
    "\n",
    "for epoch in range(RNN_N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        inputs = torch.squeeze(inputs)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        inputs = torch.squeeze(inputs)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(\"\\tTraining Loss: {:.6f} \\Test Loss: {:.6f}\".format(train_loss, test_loss))\n",
    "    if test_loss <= test_min_loss:\n",
    "        print(\"Test loss decreased ({:.6f} --> {:.6f}). Saving model...\".format(test_min_loss, test_loss))\n",
    "        torch.save(model.state_dict(), RNN_MODEL_PATH)\n",
    "        test_min_loss = test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "490b243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.91      1677\n",
      "           1       0.40      0.84      0.54       206\n",
      "\n",
      "    accuracy                           0.84      1883\n",
      "   macro avg       0.69      0.84      0.72      1883\n",
      "weighted avg       0.91      0.84      0.87      1883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "model = RNNet(EMBEDDING_DIM, RNN_HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "model.load_state_dict(torch.load(RNN_MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        inputs = torch.squeeze(inputs, dim=1)\n",
    "        y_test_pred = model(inputs)\n",
    "        _, y_test_pred = torch.max(y_test_pred, 1)\n",
    "        y_pred_tag = y_test_pred\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "\n",
    "print(classification_report(y_targ_list, y_pred_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe86eec",
   "metadata": {},
   "source": [
    "<span style=\"color:darkviolet\">\n",
    "<font size=\"5\">Gated RNN</font><br>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e40ad5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Network(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "\n",
    "        super(GRU_Network, self).__init__()\n",
    "\n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(1, self.hidden_dim))\n",
    "\n",
    "        # One time step\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "662cab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_HIDDEN_DIM = 512\n",
    "GRU_N_EPOCHS = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec37270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 7s\n",
      "\tTraining Loss: 0.026954 \\Test Loss: 0.023842\n",
      "Test loss decreased (inf --> 0.023842). Saving model...\n",
      "Epoch: 02 | Epoch Time: 0m 6s\n",
      "\tTraining Loss: 0.020906 \\Test Loss: 0.022346\n",
      "Test loss decreased (0.023842 --> 0.022346). Saving model...\n",
      "Epoch: 03 | Epoch Time: 0m 7s\n",
      "\tTraining Loss: 0.020220 \\Test Loss: 0.021780\n",
      "Test loss decreased (0.022346 --> 0.021780). Saving model...\n",
      "Epoch: 04 | Epoch Time: 0m 6s\n",
      "\tTraining Loss: 0.019413 \\Test Loss: 0.021524\n",
      "Test loss decreased (0.021780 --> 0.021524). Saving model...\n",
      "Epoch: 05 | Epoch Time: 0m 7s\n",
      "\tTraining Loss: 0.018809 \\Test Loss: 0.023243\n",
      "Epoch: 06 | Epoch Time: 0m 6s\n",
      "\tTraining Loss: 0.018164 \\Test Loss: 0.021354\n",
      "Test loss decreased (0.021524 --> 0.021354). Saving model...\n"
     ]
    }
   ],
   "source": [
    "model = GRU_Network(EMBEDDING_DIM, GRU_HIDDEN_DIM, OUTPUT_DIM)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weight))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "test_min_loss = np.inf\n",
    "\n",
    "for epoch in range(GRU_N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        inputs = torch.squeeze(inputs)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        inputs = torch.squeeze(inputs)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(\"\\tTraining Loss: {:.6f} \\Test Loss: {:.6f}\".format(train_loss, test_loss))\n",
    "    if test_loss <= test_min_loss:\n",
    "        print(\"Test loss decreased ({:.6f} --> {:.6f}). Saving model...\".format(test_min_loss, test_loss))\n",
    "        torch.save(model.state_dict(), GRU_MODEL_PATH)\n",
    "        test_min_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d4fca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.89      1677\n",
      "           1       0.36      0.76      0.49       206\n",
      "\n",
      "    accuracy                           0.82      1883\n",
      "   macro avg       0.66      0.80      0.69      1883\n",
      "weighted avg       0.90      0.82      0.85      1883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "model = GRU_Network(EMBEDDING_DIM, GRU_HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "model.load_state_dict(torch.load(GRU_MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        inputs = torch.squeeze(inputs, dim=1)\n",
    "        y_test_pred = model(inputs)\n",
    "        _, y_test_pred = torch.max(y_test_pred, 1)\n",
    "        y_pred_tag = y_test_pred\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cdd129",
   "metadata": {},
   "source": [
    "<span style=\"color:darkviolet\">\n",
    "<font size=\"5\">LSTM</font><br>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95d8d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "\n",
    "class LSTM_Network(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "\n",
    "        super(LSTM_Network, self).__init__()\n",
    "\n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(1, self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(1, self.hidden_dim))\n",
    "\n",
    "        # One time step\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea2da63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_HIDDEN_DIM = 20\n",
    "LSTM_N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e84b0dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.032762 \\Test Loss: 0.029273\n",
      "Test loss decreased (inf --> 0.029273). Saving model...\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.025426 \\Test Loss: 0.024127\n",
      "Test loss decreased (0.029273 --> 0.024127). Saving model...\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.021819 \\Test Loss: 0.022879\n",
      "Test loss decreased (0.024127 --> 0.022879). Saving model...\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.020454 \\Test Loss: 0.022493\n",
      "Test loss decreased (0.022879 --> 0.022493). Saving model...\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.019655 \\Test Loss: 0.022109\n",
      "Test loss decreased (0.022493 --> 0.022109). Saving model...\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.019404 \\Test Loss: 0.021881\n",
      "Test loss decreased (0.022109 --> 0.021881). Saving model...\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.018814 \\Test Loss: 0.022930\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.018450 \\Test Loss: 0.021499\n",
      "Test loss decreased (0.021881 --> 0.021499). Saving model...\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.018037 \\Test Loss: 0.021360\n",
      "Test loss decreased (0.021499 --> 0.021360). Saving model...\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTraining Loss: 0.017919 \\Test Loss: 0.021439\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_Network(EMBEDDING_DIM, LSTM_HIDDEN_DIM, OUTPUT_DIM)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weight))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "test_min_loss = np.inf\n",
    "\n",
    "for epoch in range(LSTM_N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        inputs = torch.squeeze(inputs)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        inputs = torch.squeeze(inputs)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(\"\\tTraining Loss: {:.6f} \\Test Loss: {:.6f}\".format(train_loss, test_loss))\n",
    "    if test_loss <= test_min_loss:\n",
    "        print(\"Test loss decreased ({:.6f} --> {:.6f}). Saving model...\".format(test_min_loss, test_loss))\n",
    "        torch.save(model.state_dict(), LSTM_MODEL_PATH)\n",
    "        test_min_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb28b03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.90      1677\n",
      "           1       0.37      0.75      0.50       206\n",
      "\n",
      "    accuracy                           0.83      1883\n",
      "   macro avg       0.67      0.80      0.70      1883\n",
      "weighted avg       0.90      0.83      0.86      1883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "model = LSTM_Network(EMBEDDING_DIM, LSTM_HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "model.load_state_dict(torch.load(LSTM_MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        inputs = torch.squeeze(inputs, dim=1)\n",
    "        y_test_pred = model(inputs)\n",
    "        _, y_test_pred = torch.max(y_test_pred, 1)\n",
    "        y_pred_tag = y_test_pred\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cff721",
   "metadata": {},
   "source": [
    "<span style=\"color:darkviolet\">\n",
    "<font size=\"5\">Bi-LSTM</font><br>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3bcb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "\n",
    "class Bi_LSTM_Network(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "\n",
    "        super(Bi_LSTM_Network, self).__init__()\n",
    "\n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(1 * 2, self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(1 * 2, self.hidden_dim))\n",
    "\n",
    "        # One time step\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9be1c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "BILSTM_HIDDEN_DIM = 20\n",
    "BILSTM_N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a4693f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 2s\n",
      "\tTraining Loss: 0.031517 \\Test Loss: 0.028087\n",
      "Test loss decreased (inf --> 0.028087). Saving model...\n",
      "Epoch: 02 | Epoch Time: 0m 2s\n",
      "\tTraining Loss: 0.024170 \\Test Loss: 0.023531\n",
      "Test loss decreased (0.028087 --> 0.023531). Saving model...\n",
      "Epoch: 03 | Epoch Time: 0m 2s\n",
      "\tTraining Loss: 0.020926 \\Test Loss: 0.023109\n",
      "Test loss decreased (0.023531 --> 0.023109). Saving model...\n",
      "Epoch: 04 | Epoch Time: 0m 2s\n",
      "\tTraining Loss: 0.020131 \\Test Loss: 0.023165\n",
      "Epoch: 05 | Epoch Time: 0m 2s\n",
      "\tTraining Loss: 0.019162 \\Test Loss: 0.022367\n",
      "Test loss decreased (0.023109 --> 0.022367). Saving model...\n",
      "Epoch: 06 | Epoch Time: 0m 3s\n",
      "\tTraining Loss: 0.019182 \\Test Loss: 0.022559\n",
      "Epoch: 07 | Epoch Time: 0m 2s\n",
      "\tTraining Loss: 0.018238 \\Test Loss: 0.023301\n",
      "Epoch: 08 | Epoch Time: 0m 2s\n",
      "\tTraining Loss: 0.017657 \\Test Loss: 0.021603\n",
      "Test loss decreased (0.022367 --> 0.021603). Saving model...\n",
      "Epoch: 09 | Epoch Time: 0m 2s\n",
      "\tTraining Loss: 0.017508 \\Test Loss: 0.021044\n",
      "Test loss decreased (0.021603 --> 0.021044). Saving model...\n",
      "Epoch: 10 | Epoch Time: 0m 2s\n",
      "\tTraining Loss: 0.017093 \\Test Loss: 0.020664\n",
      "Test loss decreased (0.021044 --> 0.020664). Saving model...\n"
     ]
    }
   ],
   "source": [
    "model = Bi_LSTM_Network(EMBEDDING_DIM, BILSTM_HIDDEN_DIM, OUTPUT_DIM)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weight))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "test_min_loss = np.inf\n",
    "\n",
    "for epoch in range(BILSTM_N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        inputs = torch.squeeze(inputs)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        inputs = torch.squeeze(inputs)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(\"\\tTraining Loss: {:.6f} \\Test Loss: {:.6f}\".format(train_loss, test_loss))\n",
    "    if test_loss <= test_min_loss:\n",
    "        print(\"Test loss decreased ({:.6f} --> {:.6f}). Saving model...\".format(test_min_loss, test_loss))\n",
    "        torch.save(model.state_dict(), BILSTM_MODEL_PATH)\n",
    "        test_min_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44a07e8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.82      0.88      1677\n",
      "           1       0.34      0.78      0.47       206\n",
      "\n",
      "    accuracy                           0.81      1883\n",
      "   macro avg       0.65      0.80      0.68      1883\n",
      "weighted avg       0.90      0.81      0.84      1883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "model = Bi_LSTM_Network(EMBEDDING_DIM, BILSTM_HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "model.load_state_dict(torch.load(BILSTM_MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        inputs = torch.squeeze(inputs, dim=1)\n",
    "        y_test_pred = model(inputs)\n",
    "        _, y_test_pred = torch.max(y_test_pred, 1)\n",
    "        y_pred_tag = y_test_pred\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('csci544')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e78917e7f90f3892e7e12462ef46781cf5994bd706032ea53be00d0b1f29dcb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
