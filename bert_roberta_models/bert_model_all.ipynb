{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2746b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.10.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.10.0)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchtext==0.10.0) (1.9.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchtext==0.10.0) (4.49.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchtext==0.10.0) (1.23.4)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchtext==0.10.0) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->torchtext==0.10.0) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->torchtext==0.10.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->torchtext==0.10.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->torchtext==0.10.0) (1.26.12)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch->torchtext==0.10.0) (4.4.0)\n",
      "Requirement already satisfied: torch==1.9.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch==1.9.0) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torchtext==0.10.0\n",
    "!pip install -U torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6dc0751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (4.24.0.dev0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (4.49.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67654b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pickle5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.0.11)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d387a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import pickle5 as pkl\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef25ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = \"tos_clauses_train.csv\"\n",
    "test_dataset_path = \"tos_clauses_dev.csv\"\n",
    "train_df = pd.read_csv(train_dataset_path, header=0)\n",
    "test_df = pd.read_csv(test_dataset_path, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4050e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(embeddings_file_path):\n",
    "    with open(embeddings_file_path, mode=\"rb\") as file:\n",
    "        data = pkl.load(file) \n",
    "    return data\n",
    "\n",
    "train_data = read_embeddings(\"train_bert_embeddings.pkl\")\n",
    "test_data = read_embeddings(\"test_bert_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee5a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TensorDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data[\"embeddings\"])\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         features = self.data[\"embeddings\"][index]\n",
    "#         label = int(self.data[\"label\"][index])\n",
    "#         return features, label\n",
    "    \n",
    "#     def __getindexlist__(self):\n",
    "#         return list(np.arange(0, len(self.data[\"embeddings\"])+1, 1))\n",
    "\n",
    "class TenDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.data1 = X\n",
    "        self.data2 = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data1[index]\n",
    "        y = self.data2[index]      \n",
    "        return torch.tensor(x), y \n",
    "\n",
    "test_len = len(train_data)\n",
    "train_len = len(test_data)\n",
    "X_train_tensor = TenDataset(train_data[\"embeddings\"], train_df[\"label\"].values)\n",
    "X_test_tensor = TenDataset(test_data[\"embeddings\"], test_df[\"label\"].values)\n",
    "\n",
    "num_of_workers = 0\n",
    "batch_size = 10\n",
    "valid_size = 0.2\n",
    "\n",
    "num_train = len(X_train_tensor)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(X_train_tensor, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_of_workers, drop_last=True)\n",
    "valid_loader = torch.utils.data.DataLoader(X_train_tensor, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_of_workers, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(X_test_tensor, batch_size=batch_size, \n",
    "    num_workers=num_of_workers, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4738103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_fair:6705\n",
      "train_unfair:826\n"
     ]
    }
   ],
   "source": [
    "train_fair = sum(train_df['label'] == 0)\n",
    "train_unfair = sum(train_df['label'] == 1)\n",
    "\n",
    "print(\"train_fair:\" + str(train_fair))\n",
    "print(\"train_unfair:\" + str(train_unfair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3351fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertClassifier(nn.Module):\n",
    "\n",
    "#     def __init__(self, dropout=0.5):\n",
    "\n",
    "#         super(BertClassifier, self).__init__()\n",
    "\n",
    "#         self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.linear_1 = nn.Linear(768, 512)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.linear_2 = nn.Linear(512, 10)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.linear_3 = nn.Linear(10, 2)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, inputs_embeds):\n",
    "\n",
    "#         _, pooled_output = self.bert(inputs_embeds = inputs_embeds,return_dict=False)\n",
    "#         dropout_output = self.dropout(pooled_output)\n",
    "#         linear_output_1 = F.relu(self.linear_1(dropout_output))\n",
    "#         linear_output_2 = F.relu(self.linear_2(linear_output_1))\n",
    "#         final_layer = F.relu(self.linear_3(linear_output_2))\n",
    "#         return final_layer\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_1 = nn.Linear(768, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs_embeds):\n",
    "\n",
    "        _, pooled_output = self.bert(inputs_embeds = inputs_embeds,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        final_layer = F.relu(self.linear_1(dropout_output))\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96eca0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(X_test_tensor, batch_size=batch_size, num_workers=num_of_workers, drop_last=True)\n",
    "\n",
    "def evaluate_after_epochs(model, test_dataloader, test_data):\n",
    "    prediction_list = []\n",
    "    actual_list = []\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in tqdm(test_dataloader):\n",
    "            test_input = torch.reshape(test_input, (batch_size, 512, 768))\n",
    "            output = curr_model(test_input)\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "            \n",
    "            prediction_list.extend(output.argmax(dim=1))\n",
    "            actual_list.extend(test_label)\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    report = classification_report(actual_list, prediction_list)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174cc69a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                   | 0/602 [00:00<?, ?it/s]/var/folders/h2/g7bly03j6zz2n3q1_1k9k6ym0000gn/T/ipykernel_10111/1438136261.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x), y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 602/602 [3:13:26<00:00, 19.28s/it]\n",
      "100%|█████████████████████████████████████████| 150/150 [06:51<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.067                 | Train Accuracy:  0.779                 | Val Loss:  0.065                 | Val Accuracy:  0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                   | 0/602 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 602/602 [3:10:17<00:00, 18.97s/it]\n",
      "100%|█████████████████████████████████████████| 150/150 [05:59<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.065                 | Train Accuracy:  0.807                 | Val Loss:  0.066                 | Val Accuracy:  0.830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                   | 0/602 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▌                                    | 54/602 [06:48<1:07:48,  7.42s/it]"
     ]
    }
   ],
   "source": [
    "def train(model, train_dataloader, val_dataloader, learning_rate, epochs, train_idx, val_idx):\n",
    "    criterion = nn.CrossEntropyLoss(weight = torch.FloatTensor([1/train_fair, 1/train_unfair]))\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "    epoch_nums_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    for epoch_num in range(epochs):\n",
    "        print(\"Epoch: \" + str(epoch_num))\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_input = torch.reshape(train_input, (batch_size, 512, 768))\n",
    "            output = model(train_input)\n",
    "                \n",
    "            batch_loss = criterion(output, train_label.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "                \n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in tqdm(val_dataloader):\n",
    "                val_input = torch.reshape(val_input, (batch_size, 512, 768))\n",
    "                output = model(val_input)\n",
    "\n",
    "                batch_loss = criterion(output, val_label.long())\n",
    "                total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "            \n",
    "        print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_idx): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_idx): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_idx): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_idx): .3f}')\n",
    "        epoch_nums_list.append(epoch_num)\n",
    "        train_loss_list.append(1- (total_acc_train / len(train_idx)))\n",
    "        val_loss_list.append(1- (total_acc_val / len(val_idx)))\n",
    "        if epoch_num in [0, 1, 2, 3, 4, 5, 6]:\n",
    "            torch.save(model.state_dict(), \"bert_mod_all_models/\" + \"bmod_\" + str(epoch_num) + \"_file.pt\")\n",
    "EPOCHS = 5\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "epoch_nums_list, train_loss_list, val_loss_list = train(model, train_loader, valid_loader, LR, EPOCHS, train_idx, valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28198819",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_nums_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mepoch_nums_list\u001b[49m, val_loss_list, color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epoch_nums_list, train_loss_list, color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend(loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupper left\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_nums_list' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(epoch_nums_list, val_loss_list, color = 'green', label = 'val loss')\n",
    "plt.plot(epoch_nums_list, train_loss_list, color = 'red', label = 'train loss')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.ylabel('epochs')\n",
    "plt.xlabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d3015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Epoch: bmod_0_file.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                   | 0/188 [00:00<?, ?it/s]/var/folders/h2/g7bly03j6zz2n3q1_1k9k6ym0000gn/T/ipykernel_1022/1438136261.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x), y\n",
      "100%|█████████████████████████████████████████| 188/188 [07:31<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.73      0.81      1675\n",
      "           1       0.17      0.46      0.25       205\n",
      "\n",
      "    accuracy                           0.70      1880\n",
      "   macro avg       0.55      0.60      0.53      1880\n",
      "weighted avg       0.84      0.70      0.75      1880\n",
      "\n",
      "Model Epoch: bmod_1_file.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                   | 0/188 [00:00<?, ?it/s]/var/folders/h2/g7bly03j6zz2n3q1_1k9k6ym0000gn/T/ipykernel_1022/1438136261.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x), y\n",
      "100%|█████████████████████████████████████████| 188/188 [07:35<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      1675\n",
      "           1       0.27      0.20      0.23       205\n",
      "\n",
      "    accuracy                           0.85      1880\n",
      "   macro avg       0.59      0.57      0.57      1880\n",
      "weighted avg       0.84      0.85      0.84      1880\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(test_dataloader, test_data):\n",
    "    path = \"bert_mod_all_models/\"\n",
    "    files = os.listdir(\"./bert_mod_all_models/\")\n",
    "    for file in files:\n",
    "        print(\"Model Epoch: \" + file)\n",
    "        curr_model = BertClassifier()\n",
    "\n",
    "        curr_model.load_state_dict(torch.load(path + file))\n",
    "        total_acc_test = 0\n",
    "        prediction_list = []\n",
    "        actual_list = []\n",
    "        with torch.no_grad():\n",
    "            for test_input, test_label in tqdm(test_dataloader):\n",
    "                test_input = torch.reshape(test_input, (batch_size, 512, 768))\n",
    "                output = curr_model(test_input)\n",
    "                acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "                total_acc_test += acc\n",
    "            \n",
    "                prediction_list.extend(output.argmax(dim=1))\n",
    "                actual_list.extend(test_label)\n",
    "                \n",
    "        print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "        report = classification_report(actual_list, prediction_list)\n",
    "        print(report)\n",
    "\n",
    "\n",
    "evaluate(test_loader, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7bc126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
